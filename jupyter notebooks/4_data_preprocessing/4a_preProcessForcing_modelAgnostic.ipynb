{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Agnostic Input Data Preprocessing in CONFLUENCE\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook focuses on the model-agnostic preprocessing steps for input data in CONFLUENCE. Model-agnostic preprocessing involves tasks that are common across different hydrological models, such as data acquisition, quality control, and initial formatting.\n",
    "\n",
    "Key steps covered in this notebook include:\n",
    "\n",
    "1. Spatial resampling of forcing data to match the model domain\n",
    "2. Calculate zonal statistics for the domain geospatial attributes \n",
    "\n",
    "In this preprocessing stage we ensure that our input data is consistent, complete, and properly formatted before we move on to model-specific preprocessing steps. By the end of this notebook, you will have clean, standardized datasets ready for further model-specific processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we import the libraries and functions we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "import yaml # type: ignore\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from utils.dataHandling_utils.agnosticPreProcessor_util import forcingResampler, geospatialStatistics # type: ignore\n",
    "from utils.dataHandling_utils.data_utils import ObservedDataProcessor # type: ignore  \n",
    "\n",
    "# Set up logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check configurations\n",
    "\n",
    "Now we should print our configuration settings and make sure that we have defined all the settings we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCING_DATASET: ERA5\n",
      "EASYMORE_CLIENT: easymore cli\n",
      "FORCING_VARIABLES: longitude,latitude,time,LWRadAtm,SWRadAtm,pptrate,airpres,airtemp,spechum,windspd\n",
      "EXPERIMENT_TIME_START: 2008-01-01 00:00\n",
      "EXPERIMENT_TIME_END: 2022-12-31 23:00\n"
     ]
    }
   ],
   "source": [
    "config_path = Path('../../0_config_files/config_active.yaml')\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "    print(f\"FORCING_DATASET: {config['FORCING_DATASET']}\")\n",
    "    print(f\"EASYMORE_CLIENT: {config['EASYMORE_CLIENT']}\")\n",
    "    print(f\"FORCING_VARIABLES: {config['FORCING_VARIABLES']}\")\n",
    "    print(f\"EXPERIMENT_TIME_START: {config['EXPERIMENT_TIME_START']}\")\n",
    "    print(f\"EXPERIMENT_TIME_END: {config['EXPERIMENT_TIME_END']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define default paths\n",
    "\n",
    "Now let's define the paths to data directories before we run the pre processing scripts and create the containing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main project directory\n",
    "data_dir = config['CONFLUENCE_DATA_DIR']\n",
    "project_dir = Path(data_dir) / f\"domain_{config['DOMAIN_NAME']}\"\n",
    "\n",
    "# Data directoris\n",
    "raw_data_dir = project_dir / 'forcing' / 'raw_data'\n",
    "basin_averaged_data = project_dir / 'forcing' / 'basin_averaged_data'\n",
    "catchment_intersection_dir = project_dir / 'shapefiles' / 'catchment_intersection'\n",
    "\n",
    "# Make sure the new directories exists\n",
    "basin_averaged_data.mkdir(parents = True, exist_ok = True)\n",
    "catchment_intersection_dir.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre process forcing data\n",
    "\n",
    "Now let's resample the forcing data onto our model domain. We use the easymore resampling tool by Gharari et al., 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize forcingReampler class\n",
    "fr = forcingResampler(config, logger)\n",
    "\n",
    "# Run resampling\n",
    "fr.run_resampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre process geospatial data\n",
    "\n",
    "Now let's calculate the zonal statistics of the geospatial attributes we need for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "# Initialize geospatialStatistics class\n",
    "gs = geospatialStatistics(config, logger)\n",
    "\n",
    "# Run resampling\n",
    "gs.run_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre process observed streamflow data\n",
    "\n",
    "Process the streamflow data into the same timestep as will run the model at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 14:30:37,714 - INFO - Processing USGS streamflow data\n",
      "2025-03-21 14:30:39,153 - INFO - Processed streamflow data saved to: /Users/amedin/Research/Confluence/CONFLUENCE_data/domain_Wolverine/observations/streamflow/preprocessed/Wolverine_streamflow_processed.csv\n",
      "2025-03-21 14:30:39,154 - INFO - Total rows in processed data: 126774\n",
      "2025-03-21 14:30:39,155 - INFO - Number of non-null values: 65770\n",
      "2025-03-21 14:30:39,156 - INFO - Number of null values: 61004\n"
     ]
    }
   ],
   "source": [
    "# Initialize ObservedDataProcessor class\n",
    "odp = ObservedDataProcessor(config, logger)\n",
    "\n",
    "# Run resampling\n",
    "odp.process_streamflow_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre process geospatial data for glaciers\n",
    "\n",
    "Now let's calculate the zonal statistics of the geospatial attributes we need for our model over the glaciers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd # type: ignore\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from shapely.geometry import mapping\n",
    "from rasterio import features\n",
    "from rasterstats import zonal_stats # type: ignore\n",
    "\n",
    "# Set up paths\n",
    "data_dir = config['CONFLUENCE_DATA_DIR']\n",
    "domain_name = config['DOMAIN_NAME']\n",
    "project_dir = Path(data_dir) / f\"domain_{config['DOMAIN_NAME']}\"\n",
    "dem_name = config['DEM_NAME']\n",
    "if dem_name == \"default\":\n",
    "    dem_name = f\"domain_{config['DOMAIN_NAME']}_elv.tif\"\n",
    "\n",
    "debris_path = project_dir / 'attributes' / 'glaciers' / f\"domain_{config['DOMAIN_NAME']}_debris_thickness.tif\"\n",
    "type_path = project_dir / 'attributes' / 'glaciers' / f\"domain_{config['DOMAIN_NAME']}_domain_type_extend.tif\"\n",
    "dem_path = project_dir / 'attributes' / 'elevation' / 'dem' / f\"domain_{config['DOMAIN_NAME']}_elv.tif\"\n",
    "\n",
    "catchment_path = project_dir / 'shapefiles' / 'catchment'\n",
    "catchment_name = config['CATCHMENT_SHP_NAME']\n",
    "if catchment_name == 'default':\n",
    "    catchment_name = f\"{config['DOMAIN_NAME']}_HRUs_{config['DOMAIN_DISCRETIZATION']}.shp\"\n",
    "catchment_gdf0 = gpd.read_file(catchment_path / catchment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:38:53,123 - INFO - Created 3 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new 'debri_mean' column\n",
      "Debris thickness statistics saved to /Users/amedin/Research/Confluence/CONFLUENCE_data/domain_Gulkana/shapefiles/catchment_intersection/with_debris_thickness/catchment_with_debris.shp\n"
     ]
    }
   ],
   "source": [
    "# Calculate debris thickness statistics\n",
    "catchment_gdf = catchment_gdf0.copy()\n",
    "with rasterio.open(debris_path) as src:\n",
    "    affine = src.transform\n",
    "    debris_data = src.read(1)\n",
    "stats = zonal_stats(catchment_gdf, debris_data, affine=affine, stats=['mean'], nodata=0)\n",
    "result_df = pd.DataFrame(stats).rename(columns={'mean': 'debri_mean_new'})\n",
    "\n",
    "if 'debri_mean' in catchment_gdf.columns:\n",
    "    print(\"Updating existing 'debri_mean' column\")\n",
    "    catchment_gdf['debri_mean'] = result_df['debri_mean_new']\n",
    "else:\n",
    "    print(\"Adding new 'debri_mean' column\")\n",
    "    catchment_gdf['debri_mean'] = result_df['debri_mean_new']\n",
    "result_df = result_df.drop(columns=['debri_mean_new'])\n",
    "\n",
    "intersect_path = project_dir / 'shapefiles' / 'catchment_intersection' / 'with_debris_thickness'\n",
    "intersect_name = 'catchment_with_debris.shp'\n",
    "intersect_path.mkdir(parents=True, exist_ok=True)\n",
    "catchment_gdf.to_file(intersect_path / intersect_name)\n",
    "\n",
    "print(f\"Debris thickness statistics saved to {intersect_path / intersect_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:38:53,157 - INFO - Created 3 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain type statistics saved to /Users/amedin/Research/Confluence/CONFLUENCE_data/domain_Gulkana/shapefiles/catchment_intersection/with_domain_type/catchment_with_domain_type.shp\n"
     ]
    }
   ],
   "source": [
    "# Calculate domain type statistics\n",
    "catchment_gdf = catchment_gdf0.copy()\n",
    "with rasterio.open(type_path) as src:\n",
    "    affine = src.transform\n",
    "    type_data = src.read(1)\n",
    "stats = zonal_stats(catchment_gdf, type_data, affine=affine, stats=['count'], categorical=True, nodata=0)\n",
    "result_df = pd.DataFrame(stats).fillna(0)\n",
    "\n",
    "def rename_column(x):\n",
    "    if x == 'count':\n",
    "        return x\n",
    "    try:\n",
    "        return f'domType_{int(float(x))}'\n",
    "    except ValueError:\n",
    "        return x\n",
    "result_df = result_df.rename(columns=rename_column)\n",
    "for col in result_df.columns:\n",
    "    if col != 'count':\n",
    "        result_df[col] = result_df[col].astype(int)\n",
    "catchment_gdf = catchment_gdf.join(result_df)\n",
    "intersect_path = project_dir / 'shapefiles' / 'catchment_intersection' / 'with_domain_type'\n",
    "intersect_name = 'catchment_with_domain_type.shp'\n",
    "intersect_path.mkdir(parents=True, exist_ok=True)\n",
    "catchment_gdf.to_file(intersect_path / intersect_name)\n",
    "\n",
    "print(f\"Domain type statistics saved to {intersect_path / intersect_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:38:53,235 - INFO - Created 3 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new 'elv_mean_1' column\n",
      "Adding new 'elv_mean_2' column\n",
      "Adding new 'elv_mean_3' column\n",
      "Adding new 'elv_mean_4' column\n",
      "Elevation statistics saved to /Users/amedin/Research/Confluence/CONFLUENCE_data/domain_Gulkana/shapefiles/catchment_intersection/with_dem_domain/catchment_with_dem_domain.shp\n"
     ]
    }
   ],
   "source": [
    "# Calculate elevation statistics, need to combine with type statistics\n",
    "catchment_gdf = catchment_gdf0.copy()\n",
    "with rasterio.open(dem_path) as src:\n",
    "    nodata_value = src.nodatavals[0]\n",
    "    if nodata_value is None:\n",
    "        nodata_value = -9999\n",
    "    affine = src.transform\n",
    "    dem_data = src.read(1)\n",
    "\n",
    "for domain_type in range(1, 6): # currently only 5 domain types\n",
    "    dem_domain_data = dem_data.copy()\n",
    "    dem_domain_data[type_data != domain_type] = nodata_value\n",
    "    if (dem_domain_data == nodata_value).all(): \n",
    "        continue # skip if no data in this domain type\n",
    "    stats = zonal_stats(catchment_gdf, dem_domain_data, affine=affine, stats=['mean'], nodata=nodata_value)\n",
    "    result_df = pd.DataFrame(stats).rename(columns={'mean': f'elv_mean_{domain_type}'})    \n",
    "    if f'elv_mean_{domain_type}' in catchment_gdf.columns:\n",
    "        print(f\"Updating existing 'elv_mean_{domain_type}' column\")\n",
    "        catchment_gdf[f'elv_mean_{domain_type}'] = result_df[f'elv_mean_{domain_type}']\n",
    "    else:\n",
    "        print(f\"Adding new 'elv_mean_{domain_type}' column\")\n",
    "        catchment_gdf[f'elv_mean_{domain_type}'] = result_df[f'elv_mean_{domain_type}']\n",
    "    result_df = result_df.drop(columns=[f'elv_mean_{domain_type}'])\n",
    "    \n",
    "    catchment_gdf = catchment_gdf.join(result_df)\n",
    "\n",
    "intersect_path = project_dir / 'shapefiles' / 'catchment_intersection' / 'with_dem_domain'\n",
    "intersect_name = 'catchment_with_dem_domain.shp'\n",
    "intersect_path.mkdir(parents=True, exist_ok=True)\n",
    "catchment_gdf.to_file(intersect_path / intersect_name)\n",
    "\n",
    "print(f\"Elevation statistics saved to {intersect_path / intersect_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new raster with HRU ID\n",
    "catchment_gdf = catchment_gdf0.copy()\n",
    "with rasterio.open(dem_path) as src:\n",
    "    affine = src.transform\n",
    "    dem_data = src.read(1)\n",
    "    meta = src.meta.copy()\n",
    "\n",
    "hru_id = catchment_gdf['HRU_ID'].values\n",
    "nodata_value = -9999\n",
    "hru_id_raster = np.ones_like(dem_data, dtype=np.int32) * nodata_value\n",
    "\n",
    "# Update the metadata to include the correct nodata value\n",
    "meta.update({\n",
    "    'dtype': 'int32',\n",
    "    'nodata': nodata_value\n",
    "})\n",
    "\n",
    "# Create a list of (geometry, value) tuples for rasterization\n",
    "shapes = [(mapping(geom), hru) for geom, hru in zip(catchment_gdf.geometry, hru_id)]\n",
    "\n",
    "# Rasterize all HRU IDs\n",
    "hru_id_raster = features.rasterize( shapes, out_shape=hru_id_raster.shape,\n",
    "    transform=affine, fill=nodata_value, all_touched=True, dtype=np.int32)\n",
    "\n",
    "# Save the raster\n",
    "hru_id_path = project_dir / 'attributes' / 'glaciers' / f\"domain_{config['DOMAIN_NAME']}_hru_id.tif\"\n",
    "with rasterio.open(hru_id_path, 'w', **meta) as dst:\n",
    "    dst.write(hru_id_raster, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confluence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
